# TLB 란 무엇인가?
TLB(Translation Lookaside Buffer)는 CPU 내부의 캐시  
더 정확히는 MMU(Memory Management Unit) 안에 있는 가상주소→물리주소 변환 캐시

🚨 CPU 레지스터(X0~X31 같은 일반 레지스터, PC, SP, 등)와는 전혀 다른 영역

캐시의 예시
- L1 캐시: 데이터/명령어 캐시
- TLB: 주소 변환 캐시

### TLB 사용처
프로세스는 가상주소를 쓰고, 메모리에 접근하는 순간마다 OS의 page table을 봐야 하는데
이걸 매번 메모리까지 가서 페이지 테이블을 읽으면 엄청 느림

OS 페이지 테이블 : OS가 각 프로세스마다 따로 가지고 있는 “큰 표(table)”
가상주소의 각 페이지 번호가 어떤 물리페이지에 매핑되는지 담은 매핑표 / virtual page → physical page 변환 규칙이 들어있는 표

-> 프로세스 수만큼 존재


### 커널 <-> 사용자 프로그램 간 cpu 제어권 전환시

이 경우 “프로세스는 바뀌지 않는다”.
TLB Flush(비우기) 하지 않는다.

이유: 같은 프로세스가 사용하는 page table이 동일

### 사용자 프로그램 <-> 사용자 프로그램간 변경 컨텍스트 스위칭시

👉 원칙적으로는 TLB Flush가 필요하다.
- 프로세스 A와 B는 서로 다른 page table 사용
- 만약 A의 주소변환 캐시(TLB)를 들고 B가 실행하면 “잘못된 물리주소를 참조해서” 메모리 전체가 꼬임

그래서 OS는 프로세스 스위칭 시 TLB을 Flush한다.

### 최적화 관점 항상 Flush를 하지는 않음 ASID(Address Space ID)

ARM, x86 등 최신 CPU는 다음 기능을 가짐:

→ ASID(또는 PCID: Process Context ID)

TLB 엔트리마다 프로세스 ID를 태깅해서
다른 프로세스의 TLB 엔트리를 자동으로 무시할 수 있게 됨.

컨텍스트 스위칭 때 TLB를 완전히 비우지 않아도 된다
OS는 CR3(context register) 변경 시 “PCID가 다르면 TLB 자동 분리” 기능을 사용


👍 성능 매우 큰 이득 (TLB miss 비용이 엄청 크기 때문)

### TLB Flush 가 왜비싼가?

TLB miss 1번 =
페이지 테이블 두세 단계 메모리 접근 → 캐시 미스 죽음 → 성능 엄청 느려짐

이에 대한 최적화
- 리눅스 스케줄러는 보통 같은 CPU에 동일 프로세스를 계속 스케줄링하려고 함 (cache affinity)
- TLB flush 빈도를 줄이려고 ASID(PCID) 기술이 도입됨


## 컨텍스트 스위칭 비용순위

🏆 컨텍스트 스위칭 비용 TOP 순위 (중요도/비용 큰 순서)


### 🥇 1등: TLB Flush에 따른 TLB Miss 비용

(ASID 없을 때 / 일부 엔트리 invalidate 시)

이게 왜 가장 비싼가?
- TLB miss → page table memory walk 발생
- 최소 1~3번 메모리 접근
  
(multi-level page table → L1/L2 캐시 미스 나면 훨씬 느림)

- CPU가 해당 명령어 실행을 멈추고 주소변환 완료를 기다림

➡️ 컨텍스트 스위칭 비용 중 가장 비싼 요소.



### 🥈 2등: CPU 캐시(Locality) 손실

이것도 무시 못함:

- L1, L2 캐시에 남아있던 이전 프로세스의 데이터가 의미없게 됨
- 새 프로세스는 자신의 코드/데이터를 다시 캐시에 채워 넣어야 함
- 캐시 미스 → 메모리에서 끌어오므로 비용 대폭 증가

➡️ 메모리 접근이 압도적으로 느리기 때문에 매우 큰 비용

즉, “캐시 지역성이 깨지는 비용”이 실제로는 TLB 비용과 함께 top-tier


### 🥉 3등: 레지스터 저장/로드 비용
- 레지스터는 CPU 내부에서 바로 저장/로드 가능
- 비용이 있지만 TLB / L1 miss 같은 “메모리 접근 비용”에 비하면 훨씬 저렴함